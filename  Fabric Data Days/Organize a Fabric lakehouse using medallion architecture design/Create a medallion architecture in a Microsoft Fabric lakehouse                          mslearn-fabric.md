---
title: "Create a medallion architecture in a Microsoft Fabric lakehouse |                         mslearn-fabric"
source: "https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/03b-medallion-lakehouse.html"
author:
published:
created: 2025-12-03
description:
tags:
  - "clippings"
---
## Microsoft Fabric ë ˆì´í¬í•˜ìš°ìŠ¤ì—ì„œ ë©”ë‹¬ë¦¬ì˜¨ ì•„í‚¤í…ì²˜ ë§Œë“¤ê¸°

ì´ ì—°ìŠµì—ì„œëŠ” ë…¸íŠ¸ë¶ì„ ì‚¬ìš©í•˜ì—¬ Fabric ë ˆì´í¬í•˜ìš°ìŠ¤ì— ë©”ë‹¬ë¦¬ì˜¨ ì•„í‚¤í…ì²˜ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤. ì‘ì—… ê³µê°„ì„ ë§Œë“¤ê³ , ë ˆì´í¬í•˜ìš°ìŠ¤ë¥¼ ìƒì„±í•˜ê³ , ë¸Œë¡ ì¦ˆ ë ˆì´ì–´ì— ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•˜ê³ , ë°ì´í„°ë¥¼ ë³€í™˜í•˜ì—¬ ì‹¤ë²„ ë¸íƒ€ í…Œì´ë¸”ì— ë¡œë“œí•˜ê³ , ë‹¤ì‹œ ë°ì´í„°ë¥¼ ë³€í™˜í•˜ì—¬ ê³¨ë“œ ë¸íƒ€ í…Œì´ë¸”ì— ë¡œë“œí•œ í›„, ì‹œë§¨í‹± ëª¨ë¸ì„ íƒìƒ‰í•˜ê³  ê´€ê³„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

**ì´ ìš´ë™ì„ ì™„ë£Œí•˜ëŠ” ë° ì•½ 45** ë¶„ì´ ì†Œìš”ë©ë‹ˆë‹¤.

> \[!ì°¸ê³ \] ì´ ì—°ìŠµì„ ì™„ë£Œí•˜ë ¤ë©´ [Microsoft Fabric í…Œë„ŒíŠ¸](https://learn.microsoft.com/fabric/get-started/fabric-trial) ì— ì•¡ì„¸ìŠ¤í•´ì•¼ í•©ë‹ˆë‹¤.

## ì‘ì—… ê³µê°„ ë§Œë“¤ê¸°

Fabricì—ì„œ ë°ì´í„° ì‘ì—…ì„ í•˜ê¸° ì „ì— Fabric í‰ê°€íŒì„ í™œì„±í™”í•˜ì—¬ ì‘ì—… ê³µê°„ì„ ë§Œë“œì„¸ìš”.

1. ë¸Œë¼ìš°ì € ì—ì„œ [Microsoft Fabric í™ˆí˜ì´ì§€](https://app.fabric.microsoft.com/home?experience=fabric-developer) ë¡œ ì´ë™í•˜ì—¬ `https://app.fabric.microsoft.com/home?experience=fabric-developer` Fabric ìê²© ì¦ëª…ì„ ì‚¬ìš©í•˜ì—¬ ë¡œê·¸ì¸í•©ë‹ˆë‹¤.
2. ì™¼ìª½ ë©”ë‰´ ëª¨ìŒì—ì„œ **ì‘ì—… ê³µê°„ì„** ì„ íƒí•˜ì„¸ìš” (ì•„ì´ì½˜ì€ ğŸ—‡ì™€ ë¹„ìŠ·í•©ë‹ˆë‹¤).
3. ì›í•˜ëŠ” ì´ë¦„ìœ¼ë¡œ ìƒˆ ì‘ì—… ê³µê°„ì„ ë§Œë“¤ê³ , **ê³ ê¸‰ ì„¹ì…˜ì—ì„œ Fabric ìš©ëŸ‰(** *í‰ê°€íŒ*, *í”„ë¦¬ë¯¸ì—„* ë˜ëŠ” *Fabric* ) ì„ í¬í•¨í•˜ëŠ” ë¼ì´ì„ ìŠ¤ ëª¨ë“œë¥¼ ì„ íƒí•©ë‹ˆë‹¤ .
4. ìƒˆ ì‘ì—… ê³µê°„ì´ ì—´ë¦¬ë©´ ë¹„ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
	[![Fabricì˜ ë¹ˆ ì‘ì—… ê³µê°„ ìŠ¤í¬ë¦°ìƒ·.](https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/Images/new-workspace.png)](https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/Images/new-workspace.png)

## ë ˆì´í¬í•˜ìš°ìŠ¤ë¥¼ ë§Œë“¤ê³  ì²­ë™ ë ˆì´ì–´ì— ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”

ì´ì œ ì‘ì—… ê³µê°„ì´ ìƒê²¼ìœ¼ë‹ˆ ë¶„ì„í•  ë°ì´í„°ë¥¼ ìœ„í•œ ë°ì´í„° ë ˆì´í¬í•˜ìš°ìŠ¤ë¥¼ ë§Œë“¤ ì°¨ë¡€ì…ë‹ˆë‹¤.

1. ë°©ê¸ˆ ë§Œë“  ì‘ì—… ê³µê°„ì—ì„œ **\+ ìƒˆ í•­ëª©** ë²„íŠ¼ì„ ì„ íƒí•˜ì—¬ **Sales** ë¼ëŠ” ì´ë¦„ì˜ ìƒˆ **Lakehouseë¥¼** ë§Œë“­ë‹ˆë‹¤.
	ì•½ 1ë¶„ í›„, ë¹„ì–´ ìˆëŠ” ìƒˆ ë ˆì´í¬í•˜ìš°ìŠ¤ê°€ ìƒì„±ë©ë‹ˆë‹¤. ë‹¤ìŒìœ¼ë¡œ, ë¶„ì„ì„ ìœ„í•´ ë°ì´í„° ë ˆì´í¬í•˜ìš°ìŠ¤ì— ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤. ì—¬ëŸ¬ ê°€ì§€ ë°©ë²•ì´ ìˆì§€ë§Œ, ì´ ì—°ìŠµì—ì„œëŠ” ë¡œì»¬ ì»´í“¨í„°(ë˜ëŠ” í•´ë‹¹ë˜ëŠ” ê²½ìš° ë© VM)ì— í…ìŠ¤íŠ¸ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•œ í›„ ë ˆì´í¬í•˜ìš°ìŠ¤ì— ì—…ë¡œë“œí•©ë‹ˆë‹¤.
2. ì´ ì—°ìŠµì— í•„ìš”í•œ ë°ì´í„° íŒŒì¼ì„ ì—ì„œ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš” `https://github.com/MicrosoftLearning/dp-data/blob/main/orders.zip`. íŒŒì¼ì„ ì¶”ì¶œí•˜ì—¬ ë¡œì»¬ ì»´í“¨í„°(ë˜ëŠ” í•´ë‹¹ë˜ëŠ” ê²½ìš° ë© VM)ì— ì›ë˜ ì´ë¦„ìœ¼ë¡œ ì €ì¥í•˜ì„¸ìš”. 3ë…„ê°„ì˜ íŒë§¤ ë°ì´í„°ê°€ í¬í•¨ëœ íŒŒì¼ 3ê°œ(2019.csv, 2020.csv, 2021.csv)ê°€ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
3. ë ˆì´í¬í•˜ìš°ìŠ¤ê°€ ìˆëŠ” ì›¹ ë¸Œë¼ìš°ì € íƒ­ìœ¼ë¡œ ëŒì•„ê°€ì„œ **íƒìƒ‰ê¸° ì°½ì˜** **íŒŒì¼** í´ë” ì— ëŒ€í•œ **... ë©”ë‰´ì—ì„œ** **ìƒˆ í•˜ìœ„ í´ë”ë¥¼** ì„ íƒ í•˜ê³  **bronze** ë¼ëŠ” ì´ë¦„ì˜ í´ë”ë¥¼ ë§Œë“­ë‹ˆë‹¤ .
4. In the **â€¦** menu for the **bronze** folder, select **Upload** and **Upload files**, and then upload the 3 files (2019.csv, 2020.csv, and 2021.csv) from your local computer (or lab VM if applicable) to the lakehouse. Use the shift key to upload all 3 files at once.
5. After the files have been uploaded, select the **bronze** folder; and verify that the files have been uploaded, as shown here:
	[![Screenshot of uploaded products.csv file in a lakehouse.](https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/Images/bronze-files.png)](https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/Images/bronze-files.png)

## Transform data and load to silver Delta table

Now that you have some data in the bronze layer of your lakehouse, you can use a notebook to transform the data and load it to a delta table in the silver layer.

1. On the **Home** page while viewing the contents of the **bronze** folder in your data lake, in the **Open notebook** menu, select **New notebook**.
	After a few seconds, a new notebook containing a single *cell* will open. Notebooks are made up of one or more cells that can contain *code* or *markdown* (formatted text).
2. When the notebook opens, rename it to `Transform data for Silver` by selecting the **Notebook xxxx** text at the top left of the notebook and entering the new name.
	[![Screenshot of a new notebook named Transform data for silver.](https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/Images/sales-notebook-rename.png)](https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/Images/sales-notebook-rename.png)
3. Select the existing cell in the notebook, which contains some simple commented-out code. Highlight and delete these two lines - you will not need this code.
	> **Note**: Notebooks enable you to run code in a variety of languages, including Python, Scala, and SQL. In this exercise, youâ€™ll use PySpark and SQL. You can also add markdown cells to provide formatted text and images to document your code.
4. **Paste** the following code into the cell:
	code
	```python
	from pyspark.sql.types import *
	    
	# Create the schema for the table
	orderSchema = StructType([
	    StructField("SalesOrderNumber", StringType()),
	    StructField("SalesOrderLineNumber", IntegerType()),
	    StructField("OrderDate", DateType()),
	    StructField("CustomerName", StringType()),
	    StructField("Email", StringType()),
	    StructField("Item", StringType()),
	    StructField("Quantity", IntegerType()),
	    StructField("UnitPrice", FloatType()),
	    StructField("Tax", FloatType())
	    ])
	    
	# Import all files from bronze folder of lakehouse
	df = spark.read.format("csv").option("header", "false").schema(orderSchema).load("Files/bronze/*.csv")
	    
	# Display the first 10 rows of the dataframe to preview your data
	display(df.head(10))
	```
5. Use the **\*\*â–·** (*Run cell*)\*\* button on the left of the cell to run the code.
	> **Note**: Since this is the first time youâ€™ve run any Spark code in this notebook, a Spark session must be started. This means that the first run can take a minute or so to complete. Subsequent runs will be quicker.
6. When the cell command has completed, **review the output** below the cell, which should look similar to this:
	| Index | SalesOrderNumber | SalesOrderLineNumber | OrderDate | CustomerName | Email | Item | Quantity | UnitPrice | Tax |
	| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
	| 1 | SO49172 | 1 | 2021-01-01 | Brian Howard | brian23@adventure-works.com | Road-250 Red, 52 | 1 | 2443.35 | 195.468 |
	| 2 | SO49173 | 1 | 2021-01-01 | Linda Alvarez | linda19@adventure-works.com | Mountain-200 Silver, 38 | 1 | 2071.4197 | 165.7136 |
	| â€¦ | â€¦ | â€¦ | â€¦ | â€¦ | â€¦ | â€¦ | â€¦ | â€¦ | â€¦ |
	The code you ran loaded the data from the CSV files in the **bronze** folder into a Spark dataframe, and then displayed the first few rows of the dataframe.
	> **Note**: You can clear, hide, and auto-resize the contents of the cell output by selecting the **â€¦** menu at the top left of the output pane.
7. Now youâ€™ll **add columns for data validation and cleanup**, using a PySpark dataframe to add columns and update the values of some of the existing columns. Use the **\+ Code** button to **add a new code block** and add the following code to the cell:
	code
	```python
	from pyspark.sql.functions import when, lit, col, current_timestamp, input_file_name
	    
	# Add columns IsFlagged, CreatedTS and ModifiedTS
	df = df.withColumn("FileName", input_file_name()) \
	    .withColumn("IsFlagged", when(col("OrderDate") < '2019-08-01',True).otherwise(False)) \
	    .withColumn("CreatedTS", current_timestamp()).withColumn("ModifiedTS", current_timestamp())
	    
	# Update CustomerName to "Unknown" if CustomerName null or empty
	df = df.withColumn("CustomerName", when((col("CustomerName").isNull() | (col("CustomerName")=="")),lit("Unknown")).otherwise(col("CustomerName")))
	```
	The first line of the code imports the necessary functions from PySpark. Youâ€™re then adding new columns to the dataframe so you can track the source file name, whether the order was flagged as being a before the fiscal year of interest, and when the row was created and modified.
	Finally, youâ€™re updating the CustomerName column to â€œUnknownâ€ if itâ€™s null or empty.
8. Run the cell to execute the code using the **\*\*â–·** (*Run cell*)\*\* button.
9. Next, youâ€™ll define the schema for the **sales\_silver** table in the sales database using Delta Lake format. Create a new code block and add the following code to the cell:
	code
	```python
	# Define the schema for the sales_silver table
	    
	from pyspark.sql.types import *
	from delta.tables import *
	    
	DeltaTable.createIfNotExists(spark) \
	    .tableName("sales.sales_silver") \
	    .addColumn("SalesOrderNumber", StringType()) \
	    .addColumn("SalesOrderLineNumber", IntegerType()) \
	    .addColumn("OrderDate", DateType()) \
	    .addColumn("CustomerName", StringType()) \
	    .addColumn("Email", StringType()) \
	    .addColumn("Item", StringType()) \
	    .addColumn("Quantity", IntegerType()) \
	    .addColumn("UnitPrice", FloatType()) \
	    .addColumn("Tax", FloatType()) \
	    .addColumn("FileName", StringType()) \
	    .addColumn("IsFlagged", BooleanType()) \
	    .addColumn("CreatedTS", DateType()) \
	    .addColumn("ModifiedTS", DateType()) \
	    .execute()
	```
10. Run the cell to execute the code using the **\*\*â–·** (*Run cell*)\*\* button.
11. Select the **â€¦** in the Tables section of the Explorer pane and select **Refresh**. You should now see the new **sales\_silver** table listed. The **â–²** (triangle icon) indicates that itâ€™s a Delta table.
	> **Note**: If you donâ€™t see the new table, wait a few seconds and then select **Refresh** again, or refresh the entire browser tab.
12. Now youâ€™re going to perform an **upsert operation** on a Delta table, updating existing records based on specific conditions and inserting new records when no match is found. Add a new code block and paste the following code:
	code
	```python
	# Update existing records and insert new ones based on a condition defined by the columns SalesOrderNumber, OrderDate, CustomerName, and Item.
	from delta.tables import *
	    
	deltaTable = DeltaTable.forPath(spark, 'Tables/sales_silver')
	    
	dfUpdates = df
	    
	deltaTable.alias('silver') \
	  .merge(
	    dfUpdates.alias('updates'),
	    'silver.SalesOrderNumber = updates.SalesOrderNumber and silver.OrderDate = updates.OrderDate and silver.CustomerName = updates.CustomerName and silver.Item = updates.Item'
	  ) \
	   .whenMatchedUpdate(set =
	    {
	          
	    }
	  ) \
	 .whenNotMatchedInsert(values =
	    {
	      "SalesOrderNumber": "updates.SalesOrderNumber",
	      "SalesOrderLineNumber": "updates.SalesOrderLineNumber",
	      "OrderDate": "updates.OrderDate",
	      "CustomerName": "updates.CustomerName",
	      "Email": "updates.Email",
	      "Item": "updates.Item",
	      "Quantity": "updates.Quantity",
	      "UnitPrice": "updates.UnitPrice",
	      "Tax": "updates.Tax",
	      "FileName": "updates.FileName",
	      "IsFlagged": "updates.IsFlagged",
	      "CreatedTS": "updates.CreatedTS",
	      "ModifiedTS": "updates.ModifiedTS"
	    }
	  ) \
	  .execute()
	```
13. Run the cell to execute the code using the **\*\*â–·** (*Run cell*)\*\* button.
	This operation is important because it enables you to update existing records in the table based on the values of specific columns, and insert new records when no match is found. This is a common requirement when youâ€™re loading data from a source system that may contain updates to existing and new records.
	You now have data in your silver delta table that is ready for further transformation and modeling.
14. After running the last cell, select the **Run** tab above the ribbon and then select **Stop session** to stop the compute resource being used by the notebook.

## Explore data in the silver layer using the SQL endpoint

Now that you have data in your silver layer, you can use the SQL analytics endpoint to explore the data and perform some basic analysis. This is useful if youâ€™re familiar with SQL and want to do some basic exploration of your data. In this exercise weâ€™re using the SQL endpoint view in Fabric, but you can use other tools like SQL Server Management Studio (SSMS) and Azure Data Explorer.

1. Navigate back to your workspace and notice that you now have several items listed. Select the **Sales SQL analytics endpoint** to open your lakehouse in the SQL analytics endpoint view.
	[![Screenshot of the SQL endpoint in a lakehouse.](https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/Images/sql-endpoint-item.png)](https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/Images/sql-endpoint-item.png)
2. Select **New SQL query** from the ribbon, which will open a SQL query editor. Note that you can rename your query using the **â€¦** menu item next to the existing query name in the Explorer pane.
	Next, youâ€™ll run two sql queries to explore the data.
3. Paste the following query into the query editor and select **Run**:
	sql
	```sql
	SELECT YEAR(OrderDate) AS Year
	    , CAST (SUM(Quantity * (UnitPrice + Tax)) AS DECIMAL(12, 2)) AS TotalSales
	FROM sales_silver
	GROUP BY YEAR(OrderDate) 
	ORDER BY YEAR(OrderDate)
	```
	This query calculates the total sales for each year in the sales\_silver table. Your results should look like this:
	[![Screenshot of the results of a SQL query in a lakehouse.](https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/Images/total-sales-sql.png)](https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/Images/total-sales-sql.png)
4. Next youâ€™ll review which customers are purchasing the most (in terms of quantity). Paste the following query into the query editor and select **Run**:
	sql
	```sql
	SELECT TOP 10 CustomerName, SUM(Quantity) AS TotalQuantity
	FROM sales_silver
	GROUP BY CustomerName
	ORDER BY TotalQuantity DESC
	```
	This query calculates the total quantity of items purchased by each customer in the sales\_silver table, and then returns the top 10 customers in terms of quantity.
	Data exploration at the silver layer is useful for basic analysis, but youâ€™ll need to transform the data further and model it into a star schema to enable more advanced analysis and reporting. Youâ€™ll do that in the next section.

## Transform data for gold layer

You have successfully taken data from your bronze layer, transformed it, and loaded it into a silver Delta table. Now youâ€™ll use a new notebook to transform the data further, model it into a star schema, and load it into gold Delta tables.

You could have done all of this in a single notebook, but for this exercise youâ€™re using separate notebooks to demonstrate the process of transforming data from bronze to silver and then from silver to gold. This can help with debugging, troubleshooting, and reuse.

1. Return to the workspace home page and create a new notebook called `Transform data for Gold`.
2. In the Explorer pane, add your **Sales** lakehouse by selecting **Add data items** and then selecting the **Sales** lakehouse you created earlier. You should see the **sales\_silver** table listed in the **Tables** section of the explorer pane.
3. In the existing code block, remove the commented text and **add the following code** to load data to your dataframe and start building your star schema, then run it:
	code
	```python
	# Load data to the dataframe as a starting point to create the gold layer
	df = spark.read.table("Sales.sales_silver")
	```
	> **Note**: If you receive a `[TooManyRequestsForCapacity]` error when running the first cell, make sure you stopped the session previously running in the first notebook.
4. **Add a new code block** and paste the following code to create your date dimension table and run it:
	code
	```python
	from pyspark.sql.types import *
	from delta.tables import*
	    
	# Define the schema for the dimdate_gold table
	DeltaTable.createIfNotExists(spark) \
	    .tableName("sales.dimdate_gold") \
	    .addColumn("OrderDate", DateType()) \
	    .addColumn("Day", IntegerType()) \
	    .addColumn("Month", IntegerType()) \
	    .addColumn("Year", IntegerType()) \
	    .addColumn("mmmyyyy", StringType()) \
	    .addColumn("yyyymm", StringType()) \
	    .execute()
	```
	> **Note**: You can run the `display(df)` command at any time to check the progress of your work. In this case, youâ€™d run â€˜display(dfdimDate\_gold)â€™ to see the contents of the dimDate\_gold dataframe.
5. In a new code block, **add and run the following code** to create a dataframe for your date dimension, **dimdate\_gold**:
	code
	```python
	from pyspark.sql.functions import col, dayofmonth, month, year, date_format
	    
	# Create dataframe for dimDate_gold
	    
	dfdimDate_gold = df.dropDuplicates(["OrderDate"]).select(col("OrderDate"), \
	        dayofmonth("OrderDate").alias("Day"), \
	        month("OrderDate").alias("Month"), \
	        year("OrderDate").alias("Year"), \
	        date_format(col("OrderDate"), "MMM-yyyy").alias("mmmyyyy"), \
	        date_format(col("OrderDate"), "yyyyMM").alias("yyyymm"), \
	    ).orderBy("OrderDate")
	# Display the first 10 rows of the dataframe to preview your data
	display(dfdimDate_gold.head(10))
	```
6. Youâ€™re separating the code out into new code blocks so that you can understand and watch whatâ€™s happening in the notebook as you transform the data. In another new code block, **add and run the following code** to update the date dimension as new data comes in:
	code
	```python
	from delta.tables import *
	    
	deltaTable = DeltaTable.forPath(spark, 'Tables/dimdate_gold')
	    
	dfUpdates = dfdimDate_gold
	    
	deltaTable.alias('gold') \
	  .merge(
	    dfUpdates.alias('updates'),
	    'gold.OrderDate = updates.OrderDate'
	  ) \
	   .whenMatchedUpdate(set =
	    {
	          
	    }
	  ) \
	 .whenNotMatchedInsert(values =
	    {
	      "OrderDate": "updates.OrderDate",
	      "Day": "updates.Day",
	      "Month": "updates.Month",
	      "Year": "updates.Year",
	      "mmmyyyy": "updates.mmmyyyy",
	      "yyyymm": "updates.yyyymm"
	    }
	  ) \
	  .execute()
	```
	The date dimension is now set up. Now youâ€™ll create your customer dimension.
7. To build out the customer dimension table, **add a new code block**, paste and run the following code:
	code
	```python
	from pyspark.sql.types import *
	from delta.tables import *
	    
	# Create customer_gold dimension delta table
	DeltaTable.createIfNotExists(spark) \
	    .tableName("sales.dimcustomer_gold") \
	    .addColumn("CustomerName", StringType()) \
	    .addColumn("Email",  StringType()) \
	    .addColumn("First", StringType()) \
	    .addColumn("Last", StringType()) \
	    .addColumn("CustomerID", LongType()) \
	    .execute()
	```
8. In a new code block, **add and run the following code** to drop duplicate customers, select specific columns, and split the â€œCustomerNameâ€ column to create â€œFirstâ€ and â€œLastâ€ name columns:
	code
	```python
	from pyspark.sql.functions import col, split
	    
	# Create customer_silver dataframe
	    
	dfdimCustomer_silver = df.dropDuplicates(["CustomerName","Email"]).select(col("CustomerName"),col("Email")) \
	    .withColumn("First",split(col("CustomerName"), " ").getItem(0)) \
	    .withColumn("Last",split(col("CustomerName"), " ").getItem(1)) 
	    
	# Display the first 10 rows of the dataframe to preview your data
	display(dfdimCustomer_silver.head(10))
	```
	Here you have created a new DataFrame dfdimCustomer\_silver by performing various transformations such as dropping duplicates, selecting specific columns, and splitting the â€œCustomerNameâ€ column to create â€œFirstâ€ and â€œLastâ€ name columns. The result is a DataFrame with cleaned and structured customer data, including separate â€œFirstâ€ and â€œLastâ€ name columns extracted from the â€œCustomerNameâ€ column.
9. Next weâ€™ll **create the ID column for our customers**. In a new code block, paste and run the following:
	code
	```python
	from pyspark.sql.functions import monotonically_increasing_id, col, when, coalesce, max, lit
	    
	dfdimCustomer_temp = spark.read.table("Sales.dimCustomer_gold")
	    
	MAXCustomerID = dfdimCustomer_temp.select(coalesce(max(col("CustomerID")),lit(0)).alias("MAXCustomerID")).first()[0]
	    
	dfdimCustomer_gold = dfdimCustomer_silver.join(dfdimCustomer_temp,(dfdimCustomer_silver.CustomerName == dfdimCustomer_temp.CustomerName) & (dfdimCustomer_silver.Email == dfdimCustomer_temp.Email), "left_anti")
	    
	dfdimCustomer_gold = dfdimCustomer_gold.withColumn("CustomerID",monotonically_increasing_id() + MAXCustomerID + 1)
	# Display the first 10 rows of the dataframe to preview your data
	display(dfdimCustomer_gold.head(10))
	```
	Here youâ€™re cleaning and transforming customer data (dfdimCustomer\_silver) by performing a left anti join to exclude duplicates that already exist in the dimCustomer\_gold table, and then generating unique CustomerID values using the monotonically\_increasing\_id() function.
10. Now youâ€™ll ensure that your customer table remains up-to-date as new data comes in. **In a new code block**, paste and run the following:
	code
	```python
	from delta.tables import *
	deltaTable = DeltaTable.forPath(spark, 'Tables/dimcustomer_gold')
	    
	dfUpdates = dfdimCustomer_gold
	    
	deltaTable.alias('gold') \
	  .merge(
	    dfUpdates.alias('updates'),
	    'gold.CustomerName = updates.CustomerName AND gold.Email = updates.Email'
	  ) \
	   .whenMatchedUpdate(set =
	    {
	          
	    }
	  ) \
	 .whenNotMatchedInsert(values =
	    {
	      "CustomerName": "updates.CustomerName",
	      "Email": "updates.Email",
	      "First": "updates.First",
	      "Last": "updates.Last",
	      "CustomerID": "updates.CustomerID"
	    }
	  ) \
	  .execute()
	```
11. Now youâ€™ll **repeat those steps to create your product dimension**. In a new code block, paste and run the following:
	code
	```python
	from pyspark.sql.types import *
	from delta.tables import *
	    
	DeltaTable.createIfNotExists(spark) \
	    .tableName("sales.dimproduct_gold") \
	    .addColumn("ItemName", StringType()) \
	    .addColumn("ItemID", LongType()) \
	    .addColumn("ItemInfo", StringType()) \
	    .execute()
	```
12. **Add another code block** to create the **product\_silver** dataframe.
	code
	```python
	from pyspark.sql.functions import col, split, lit, when
	    
	# Create product_silver dataframe
	    
	dfdimProduct_silver = df.dropDuplicates(["Item"]).select(col("Item")) \
	    .withColumn("ItemName",split(col("Item"), ", ").getItem(0)) \
	    .withColumn("ItemInfo",when((split(col("Item"), ", ").getItem(1).isNull() | (split(col("Item"), ", ").getItem(1)=="")),lit("")).otherwise(split(col("Item"), ", ").getItem(1))) 
	    
	# Display the first 10 rows of the dataframe to preview your data
	display(dfdimProduct_silver.head(10))
	```
13. Now youâ€™ll create IDs for your **dimProduct\_gold table**. Add the following syntax to a new code block and run it:
	code
	```python
	from pyspark.sql.functions import monotonically_increasing_id, col, lit, max, coalesce
	    
	#dfdimProduct_temp = dfdimProduct_silver
	dfdimProduct_temp = spark.read.table("Sales.dimProduct_gold")
	    
	MAXProductID = dfdimProduct_temp.select(coalesce(max(col("ItemID")),lit(0)).alias("MAXItemID")).first()[0]
	    
	dfdimProduct_gold = dfdimProduct_silver.join(dfdimProduct_temp,(dfdimProduct_silver.ItemName == dfdimProduct_temp.ItemName) & (dfdimProduct_silver.ItemInfo == dfdimProduct_temp.ItemInfo), "left_anti")
	    
	dfdimProduct_gold = dfdimProduct_gold.withColumn("ItemID",monotonically_increasing_id() + MAXProductID + 1)
	    
	# Display the first 10 rows of the dataframe to preview your data
	display(dfdimProduct_gold.head(10))
	```
	This calculates the next available product ID based on the current data in the table, assigns these new IDs to the products, and then displays the updated product information.
14. Similar to what youâ€™ve done with your other dimensions, you need to ensure that your product table remains up-to-date as new data comes in. **In a new code block**, paste and run the following:
	code
	```python
	from delta.tables import *
	    
	deltaTable = DeltaTable.forPath(spark, 'Tables/dimproduct_gold')
	            
	dfUpdates = dfdimProduct_gold
	            
	deltaTable.alias('gold') \
	  .merge(
	        dfUpdates.alias('updates'),
	        'gold.ItemName = updates.ItemName AND gold.ItemInfo = updates.ItemInfo'
	        ) \
	        .whenMatchedUpdate(set =
	        {
	               
	        }
	        ) \
	        .whenNotMatchedInsert(values =
	         {
	          "ItemName": "updates.ItemName",
	          "ItemInfo": "updates.ItemInfo",
	          "ItemID": "updates.ItemID"
	          }
	          ) \
	          .execute()
	```
	Now that you have your dimensions built out, the final step is to create the fact table.
15. **In a new code block**, paste and run the following code to create the **fact table**:
	code
	```python
	from pyspark.sql.types import *
	from delta.tables import *
	    
	DeltaTable.createIfNotExists(spark) \
	    .tableName("sales.factsales_gold") \
	    .addColumn("CustomerID", LongType()) \
	    .addColumn("ItemID", LongType()) \
	    .addColumn("OrderDate", DateType()) \
	    .addColumn("Quantity", IntegerType()) \
	    .addColumn("UnitPrice", FloatType()) \
	    .addColumn("Tax", FloatType()) \
	    .execute()
	```
16. **In a new code block**, paste and run the following code to create a **new dataframe** to combine sales data with customer and product information include customer ID, item ID, order date, quantity, unit price, and tax:
	code
	```python
	from pyspark.sql.functions import col
	    
	dfdimCustomer_temp = spark.read.table("Sales.dimCustomer_gold")
	dfdimProduct_temp = spark.read.table("Sales.dimProduct_gold")
	    
	df = df.withColumn("ItemName",split(col("Item"), ", ").getItem(0)) \
	    .withColumn("ItemInfo",when((split(col("Item"), ", ").getItem(1).isNull() | (split(col("Item"), ", ").getItem(1)=="")),lit("")).otherwise(split(col("Item"), ", ").getItem(1))) \
	    
	    
	# Create Sales_gold dataframe
	    
	dffactSales_gold = df.alias("df1").join(dfdimCustomer_temp.alias("df2"),(df.CustomerName == dfdimCustomer_temp.CustomerName) & (df.Email == dfdimCustomer_temp.Email), "left") \
	        .join(dfdimProduct_temp.alias("df3"),(df.ItemName == dfdimProduct_temp.ItemName) & (df.ItemInfo == dfdimProduct_temp.ItemInfo), "left") \
	    .select(col("df2.CustomerID") \
	        , col("df3.ItemID") \
	        , col("df1.OrderDate") \
	        , col("df1.Quantity") \
	        , col("df1.UnitPrice") \
	        , col("df1.Tax") \
	    ).orderBy(col("df1.OrderDate"), col("df2.CustomerID"), col("df3.ItemID"))
	    
	# Display the first 10 rows of the dataframe to preview your data
	    
	display(dffactSales_gold.head(10))
	```
17. **ì´ì œ ìƒˆ ì½”ë“œ ë¸”ë¡** ì—ì„œ ë‹¤ìŒ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì—¬ íŒë§¤ ë°ì´í„°ê°€ ìµœì‹  ìƒíƒœë¡œ ìœ ì§€ë˜ë„ë¡ í•˜ì„¸ìš” .
	ì•”í˜¸
	```python
	from delta.tables import *
	    
	deltaTable = DeltaTable.forPath(spark, 'Tables/factsales_gold')
	    
	dfUpdates = dffactSales_gold
	    
	deltaTable.alias('gold') \
	  .merge(
	    dfUpdates.alias('updates'),
	    'gold.OrderDate = updates.OrderDate AND gold.CustomerID = updates.CustomerID AND gold.ItemID = updates.ItemID'
	  ) \
	   .whenMatchedUpdate(set =
	    {
	          
	    }
	  ) \
	 .whenNotMatchedInsert(values =
	    {
	      "CustomerID": "updates.CustomerID",
	      "ItemID": "updates.ItemID",
	      "OrderDate": "updates.OrderDate",
	      "Quantity": "updates.Quantity",
	      "UnitPrice": "updates.UnitPrice",
	      "Tax": "updates.Tax"
	    }
	  ) \
	  .execute()
	```
	ì—¬ê¸°ì„œëŠ” Delta Lakeì˜ ë³‘í•© ì‘ì—…ì„ ì‚¬ìš©í•˜ì—¬ factsales\_gold í…Œì´ë¸”ì„ ìƒˆë¡œìš´ íŒë§¤ ë°ì´í„°(dffactSales\_gold)ë¡œ ë™ê¸°í™”í•˜ê³  ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. ì´ ì‘ì—…ì€ ê¸°ì¡´ ë°ì´í„°(silver í…Œì´ë¸”)ì™€ ìƒˆ ë°ì´í„°(updates DataFrame)ì˜ ì£¼ë¬¸ ë‚ ì§œ, ê³ ê° ID, í’ˆëª© IDë¥¼ ë¹„êµí•˜ì—¬ ì¼ì¹˜í•˜ëŠ” ë ˆì½”ë“œë¥¼ ì—…ë°ì´íŠ¸í•˜ê³  í•„ìš”ì— ë”°ë¼ ìƒˆ ë ˆì½”ë“œë¥¼ ì‚½ì…í•©ë‹ˆë‹¤.

ì´ì œ ë³´ê³  ë° ë¶„ì„ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” íë ˆì´íŒ…ë˜ê³  ëª¨ë¸ë§ëœ **ê³¨ë“œ** ë ˆì´ì–´ê°€ ìƒê²¼ìŠµë‹ˆë‹¤.

## (ì„ íƒ ì‚¬í•­) ì˜ë¯¸ ëª¨ë¸ì„ ë§Œë“­ë‹ˆë‹¤.

**ì°¸ê³ **: ì´ ì‘ì—…ì€ ì „ì ìœ¼ë¡œ ì„ íƒ ì‚¬í•­ì´ì§€ë§Œ ì˜ë¯¸ ëª¨ë¸ì„ ë§Œë“¤ê³  í¸ì§‘í•˜ë ¤ë©´ Power BI ë¼ì´ì„ ìŠ¤ ë˜ëŠ” Fabric F64 SKUê°€ í•„ìš”í•©ë‹ˆë‹¤.

ì´ì œ ì‘ì—… ê³µê°„ì—ì„œ ê³¨ë“œ ë ˆì´ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ê³  ë°ì´í„°ë¥¼ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‘ì—… ê³µê°„ì—ì„œ ì˜ë¯¸ ëª¨ë¸ì— ì§ì ‘ ì•¡ì„¸ìŠ¤í•˜ì—¬ ë³´ê³ ë¥¼ ìœ„í•œ ê´€ê³„ ë° ì¸¡ì •ê°’ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**ë ˆì´í¬í•˜ìš°ìŠ¤ë¥¼ ìƒì„±í•  ë•Œ ìë™ìœ¼ë¡œ ìƒì„±ë˜ëŠ” ê¸°ë³¸ ì˜ë¯¸ ëª¨ë¸ì„** ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ . ì´ ì—°ìŠµì—ì„œ ìƒì„±í•œ ê³¨ë“œ í…Œì´ë¸”ì„ í¬í•¨í•˜ëŠ” ìƒˆë¡œìš´ ì˜ë¯¸ ëª¨ë¸ì„ íƒìƒ‰ê¸°ì—ì„œ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.

1. ì‘ì—… ê³µê°„ì—ì„œ **Sales** Lakehouseë¡œ ì´ë™í•©ë‹ˆë‹¤.
2. íƒìƒ‰ê¸° ë³´ê¸°ì˜ ë¦¬ë³¸ì—ì„œ **ìƒˆ ì˜ë¯¸ ëª¨ë¸ì„** ì„ íƒí•©ë‹ˆë‹¤.
3. ìƒˆë¡œìš´ ì˜ë¯¸ ëª¨ë¸ì— **Sales\_Goldë¼ëŠ”** ì´ë¦„ì„ ì§€ì •í•©ë‹ˆë‹¤.
4. ì˜ë¯¸ ëª¨ë¸ì— í¬í•¨í•  ë³€í™˜ëœ ê³¨ë“œ í…Œì´ë¸”ì„ ì„ íƒí•˜ê³  **í™•ì¸ì„** ì„ íƒí•©ë‹ˆë‹¤.
	- ë”¤ë°ì´íŠ¸\_ê³¨ë“œ
	- dimcustomer\_gold
	- dimproduct\_gold
	- ì‚¬ì‹¤ì„¸ì¼ì¦ˆ\_ê³¨ë“œ
	ì´ë ‡ê²Œ í•˜ë©´ Fabricì—ì„œ ì˜ë¯¸ ëª¨ë¸ì´ ì—´ë¦¬ê³  ì—¬ê¸°ì„œ ë‹¤ìŒê³¼ ê°™ì´ ê´€ê³„ì™€ ì¸¡ì •ê°’ì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
	[![Screenshot of a semantic model in Fabric.](https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/Images/dataset-relationships.png)](https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/Images/dataset-relationships.png)

ì—¬ê¸°ì—ì„œ ë³¸ì¸ì´ë‚˜ ë°ì´í„° íŒ€ êµ¬ì„±ì›ì€ ë ˆì´í¬í•˜ìš°ìŠ¤ì˜ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë³´ê³ ì„œì™€ ëŒ€ì‹œë³´ë“œë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë³´ê³ ì„œëŠ” ë ˆì´í¬í•˜ìš°ìŠ¤ì˜ ê³¨ë“œ ë ˆì´ì–´ì— ì§ì ‘ ì—°ê²°ë˜ë¯€ë¡œ í•­ìƒ ìµœì‹  ë°ì´í„°ë¥¼ ë°˜ì˜í•©ë‹ˆë‹¤.

## ìì› ì •ë¦¬

ì´ ì—°ìŠµì—ì„œëŠ” Microsoft Fabric ë ˆì´í¬í•˜ìš°ìŠ¤ì—ì„œ ë©”ë‹¬ë¦¬ì˜¨ ì•„í‚¤í…ì²˜ë¥¼ ë§Œë“œëŠ” ë°©ë²•ì„ ì•Œì•„ë³´ì•˜ìŠµë‹ˆë‹¤.

í˜¸ìˆ«ê°€ ì£¼íƒ íƒí—˜ì„ ë§ˆì³¤ë‹¤ë©´ ì´ ì—°ìŠµì„ ìœ„í•´ ë§Œë“  ì‘ì—… ê³µê°„ì„ ì‚­ì œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

1. ì™¼ìª½ ë§‰ëŒ€ì—ì„œ ì‘ì—… ê³µê°„ ì•„ì´ì½˜ì„ ì„ íƒí•˜ë©´ í•´ë‹¹ ì‘ì—… ê³µê°„ì— í¬í•¨ëœ ëª¨ë“  í•­ëª©ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
2. **ë„êµ¬ ëª¨ìŒì˜ â€¦** ë©”ë‰´ ì—ì„œ **ì‘ì—… ê³µê°„ ì„¤ì •ì„** ì„ íƒí•©ë‹ˆë‹¤ .
3. **ì¼ë°˜** ì„¹ì…˜ ì—ì„œ **ì´ ì‘ì—… ê³µê°„ ì œê±°ë¥¼** ì„ íƒí•©ë‹ˆë‹¤.